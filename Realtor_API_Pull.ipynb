{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e06ad3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pb \n",
    "import requests\n",
    "import pyspark\n",
    "from pyspark import SparkConf                                                                                                                 \n",
    "from pyspark.context import SparkContext                                                                                                      \n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c250f709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!PIP install pyspark\n",
    "#!PIP install Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb3c132a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-65c47e0f65ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \"\"\"\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m# load api keys file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdf_api_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FileStore/tables/api_keys.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#df_api_keys = spark.read.csv('FileStore/tables/api_keys.csv').toPandas()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_api_key(api_key_id = \"Realtor\"):\n",
    "  \"\"\"\n",
    "  Get the api key for website accessing.\n",
    "\n",
    "  Table of key type and key value for privacy.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  @api_key_id [string]: Key value in dataframe\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  [string]: API Key\n",
    "\n",
    "  \"\"\"\n",
    "  # load api keys file\n",
    "df_api_keys = spark.read.format('csv').options(header='true', inferSchema='true').load('FileStore/tables/api_keys.csv').toPandas()\n",
    "\n",
    "#df_api_keys = spark.read.csv('FileStore/tables/api_keys.csv').toPandas()\n",
    "  \n",
    "    # return api key if in dataset\n",
    "try:\n",
    "    # get api key from id\n",
    "    api_key = df_api_keys.loc[df_api_keys['Id'] == api_key_id]['Key'].iloc[0] # get key by id\n",
    "    # return api key\n",
    "    return api_key\n",
    "\n",
    "except IndexError:\n",
    "    # get api key id list\n",
    "    api_key_id_list = df_api_keys['Id'].unique().tolist()\n",
    "    # print error message\n",
    "    print('Cannot map key. Api key id must be one of the following options {0}'.format(api_key_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7aca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def api_property_list_for_sale(api_key, city, state, prop_type, limit=200):\n",
    "  # url for api\n",
    "  url = \"https://realtor-com-real-estate.p.rapidapi.com/for-sale\"\n",
    "\n",
    "  # enter parameters\n",
    "  querystring = {\n",
    "    \"sort\":\"relevance\",\n",
    "    \"city\":city,\n",
    "    \"offset\":\"0\",\n",
    "    \"limit\":limit,\n",
    "    \"state_code\":state,\n",
    "    \"prop_type\":prop_type\n",
    "  }\n",
    "\n",
    "  # header\n",
    "  headers = {\n",
    "    'x-rapidapi-host': \"realtor.p.rapidapi.com\",\n",
    "    'x-rapidapi-key': api_key\n",
    "  }\n",
    "\n",
    "  # response\n",
    "  response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "  return response.json() # json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0758963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_list_for_sale_response(response_json):\n",
    "    \"\"\"\n",
    "    Process the list for sale API response.\n",
    "\n",
    "    Convert each listing to a dataframe, append to a list, and concatenate to one dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    @response_json [dictionary]: API response for list for sale\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    [dataframe] Dataframe of all list for sale responses\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # empty dataframe\n",
    "    dataframe_list = []\n",
    "\n",
    "    # iterate through each for sale listing\n",
    "    for l in response_json['properties']:\n",
    "\n",
    "        # convert each listing to dataframe\n",
    "        _temp_df = pd.DataFrame.from_dict(l, orient='index').T\n",
    "\n",
    "        # append to dataframe list for all listings\n",
    "        dataframe_list.append(_temp_df)\n",
    "\n",
    "    # concatenate all dataframes, for missing col values enter null value\n",
    "    return pd.concat(dataframe_list, axis=0, ignore_index=True, sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c269e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# api key to accessd ata\n",
    "realtor_api_key = get_api_key(api_key_id = \"Realtor\")\n",
    "city = \"Nashville\"\n",
    "state = \"TN\"\n",
    "prop_type = \"single_family\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5f09a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

