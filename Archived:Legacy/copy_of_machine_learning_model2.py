# -*- coding: utf-8 -*-
"""Copy of Machine_Learning_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ShYqjOI1wF3Av5WOU2HFRmXLwN18F46N
"""

pip install psycopg2-binary

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler,OneHotEncoder
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
import tensorflow as tf
import sqlalchemy as db
from sqlalchemy.ext.automap import automap_base

engine = db.create_engine('postgresql://postgres:finalproject@finalproject.cfnecwioimp0.us-east-2.rds.amazonaws.com:5432/finalProject')
with engine.connect() as conn, conn.begin():
    df = pd.read_sql("""select * from redfin_df;""", conn)
df

pd.set_option("display.max_rows",15, "display.max_columns", None)

ml_df = df
ml_df = ml_df.drop([
    'sold_date',
    'address',
    'state_or_province',
    #'city',
    'url',
    'mls_number',
    'latitude',
    'longitude',
    'neighborhood'
    ], axis =1)
ml_df.dtypes

convert_dict = {'zip_code': object,
                'beds': int,
                'square_feet': int,
                'lot_size': int,
                'year_built': object,
                'price_per_square_feet': int,
               }
  
ml_df = ml_df.astype(convert_dict)
object1 = ml_df.dtypes[ml_df.dtypes == "object"].index.to_list()
enc = OneHotEncoder(sparse=False)

encode_df = pd.DataFrame(enc.fit_transform(ml_df[object1]))
encode_df.columns = enc.get_feature_names(object1)
encode_df

ml_encode_df = ml_df.merge(encode_df,left_index=True, right_index=True)
ml_encode_df = ml_encode_df.drop(object1,1)
ml_encode_df

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
  
# Scaling the Price column of the created dataFrame and storing
# the result in ScaledPrice Column
ml_encode_df[["scaledSF", "scaledLS"]] = scaler.fit_transform(ml_encode_df[["square_feet","lot_size",]])
ml_encode_df = ml_encode_df.drop(["square_feet","lot_size"], axis= 1)

y = ml_encode_df["price"].values
X = ml_encode_df.drop(["price"],1).values

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

#Define the model - deep neural net
number_input_features = len(X_train[0])

nn = tf.keras.models.Sequential()

# First hidden layer
nn.add(
    tf.keras.layers.Dense(units=50, input_dim=number_input_features, activation="linear")
)
nn.add(
    tf.keras.layers.Dense(units=40, activation="relu")
)

nn.add(
    tf.keras.layers.Dense(units=30, activation="linear")
)
nn.add(
    tf.keras.layers.Dense(units=20, activation="linear")
)

nn.add(
    tf.keras.layers.Dense(units=10, activation="linear")
)
# Second hidden layer
nn.add(tf.keras.layers.Dense(units=8, kernel_initializer='normal'))



# Output layer
nn.add(tf.keras.layers.Dense(units=1))

# Check the structure of the model
nn.summary()

# Compile the model
nn.compile(loss='mean_squared_error', optimizer="adam", metrics=["accuracy"])

# Train the model
fit_model = nn.fit(X_train,y_train,epochs=300)

prediction = nn.predict(X_test)
pred = pd.DataFrame({ 'actual': y_test})
pred['prediction'] = prediction
pred['error'] =pred.prediction - pred.actual
pred['error_abs'] = abs(pred['error'])
pred.error_abs.mean()

